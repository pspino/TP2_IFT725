{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Dropout [1] est une technique de régularisation qui consiste à forcer aléatoirement à zéro certains neurones lors de la propagation avant. Pour cet exercice, vous serez appelé à coder une couche de dropout et de l'incorporer à votre réseau pleinement connecté.\n",
    "\n",
    "[1] Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "run the following from the ift725 directory and try again:\n",
      "python setup.py build_ext --inplace\n",
      "You may also need to restart your iPython kernel\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ift725.classifiers.fc_net import *\n",
    "from ift725.data_utils import get_CIFAR10_data\n",
    "from ift725.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from ift725.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout: propagation avant\n",
    "Dans le fichier `ift725/layers.py`, codez la propagation avant du dropout. Puisque dropout se comporte différemment en entraînement qu'en test, assurez-vous que les deux modes fonctionnent bien.\n",
    "\n",
    "Exécutez la cellule que voici et assurez-vous que la moyenne de out_train soit la même que out_test.\n",
    "\n",
    "NOTE : vous devez implémenter du \"inverse dropout\".  Pour plus de détail, voir https://deepnotes.io/dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Running tests with p =  0.3\n",
      "Mean of input:  10.00263693959339\n",
      "Mean of train-time output:  10.001183275245882\n",
      "Mean of test-time output:  10.00263693959339\n",
      "Fraction of train-time output set to zero:  0.29988\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.6\n",
      "Mean of input:  10.00263693959339\n",
      "Mean of train-time output:  9.997410809432022\n",
      "Mean of test-time output:  10.00263693959339\n",
      "Fraction of train-time output set to zero:  0.600132\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.75\n",
      "Mean of input:  10.00263693959339\n",
      "Mean of train-time output:  9.973079155566998\n",
      "Mean of test-time output:  10.00263693959339\n",
      "Fraction of train-time output set to zero:  0.7509\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = np.random.randn(500, 500) + 10\n",
    "\n",
    "for p in [0.3, 0.6, 0.75]:\n",
    "  out_train, _ = forward_inverted_dropout(x, {'mode': 'train', 'p': p})\n",
    "  out_test, _ = forward_inverted_dropout(x, {'mode': 'test', 'p': p})\n",
    "\n",
    "  print('Running tests with p = ', p)\n",
    "  print('Mean of input: ', x.mean())\n",
    "  print('Mean of train-time output: ', out_train.mean())\n",
    "  print('Mean of test-time output: ', out_test.mean())\n",
    "  print('Fraction of train-time output set to zero: ', (out_train == 0).mean())\n",
    "  print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout: rétro-propagation\n",
    "Dans le fichier `ift725/layers.py`, codez la rétro-propagation du dropout. Vous pourrez par la suite tester votre code avec la cellule que voici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "dx relative error:  1.8929052044150925e-11\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dropout_param = {'mode': 'train', 'p': 0.8, 'seed': 123}\n",
    "out, cache = forward_inverted_dropout(x, dropout_param)\n",
    "dx = backward_inverted_dropout(dout, cache)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: forward_inverted_dropout(xx, dropout_param)[0], x, dout)\n",
    "\n",
    "# Error should be below 1e-10\n",
    "print('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau multi-couches avec Dropout\n",
    "Dans le fichier `ift725/classifiers/fc_net.py`, modifiez votre code afin d'y incorporer dropout. Plus particulièrement, si le constructeur du réseau reçoit une valeur non nulle pour le paramètre `dropout`, le réseau devrait ajouter du dropout après chaque ReLU. Vous pouvez tester votre code avec la cellule que voici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Running check with dropout =  0\n",
      "Initial loss:  2.305194827398786\n",
      "W1 relative error: 8.47e-07\n",
      "W2 relative error: 1.50e-05\n",
      "W3 relative error: 1.61e-07\n",
      "b1 relative error: 2.94e-06\n",
      "b2 relative error: 1.20e-07\n",
      "b3 relative error: 1.52e-10\n",
      "\n",
      "Running check with dropout =  0.25\n",
      "Initial loss:  2.29898614757146\n",
      "W1 relative error: 9.74e-07\n",
      "W2 relative error: 4.12e-08\n",
      "W3 relative error: 3.86e-08\n",
      "b1 relative error: 2.01e-08\n",
      "b2 relative error: 7.34e-09\n",
      "b3 relative error: 9.60e-11\n",
      "\n",
      "Running check with dropout =  0.5\n",
      "Initial loss:  2.3035667586595423\n",
      "W1 relative error: 1.14e-06\n",
      "W2 relative error: 7.42e-08\n",
      "W3 relative error: 1.29e-08\n",
      "b1 relative error: 8.76e-08\n",
      "b2 relative error: 1.20e-09\n",
      "b3 relative error: 1.46e-10\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for dropout in [0, 0.25, 0.5]:\n",
    "  print('Running check with dropout = ', dropout)\n",
    "  model = FullyConnectedNeuralNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            weight_scale=5e-2, dtype=np.float64,\n",
    "                            dropout=dropout, seed=123)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "    \n",
    "  # Error should be below 1e-5\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expérimentation\n",
    "Ici nous entrainerons 2 réseaux de neurones avec 500 données: l'un utilisera du dropout et l'autre non. Nous pourrons alors visualiser les justesses obtenues en entraînement et en validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0\n",
      "(Iteration 1 / 125) loss: 11.505750\n",
      "(Epoch 0 / 25) train acc: 0.196000; val_acc: 0.154000\n",
      "(Epoch 1 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 2 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 3 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 4 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 5 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 6 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 7 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 8 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 9 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 10 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 11 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 12 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 13 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 14 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 15 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 16 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 17 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 18 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 19 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 20 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Iteration 101 / 125) loss: nan\n",
      "(Epoch 21 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 22 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 23 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 24 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "(Epoch 25 / 25) train acc: 0.102000; val_acc: 0.087000\n",
      "0.25\n",
      "(Iteration 1 / 125) loss: 10.291531\n",
      "(Epoch 0 / 25) train acc: 0.226000; val_acc: 0.189000\n",
      "(Epoch 1 / 25) train acc: 0.376000; val_acc: 0.218000\n",
      "(Epoch 2 / 25) train acc: 0.430000; val_acc: 0.223000\n",
      "(Epoch 3 / 25) train acc: 0.434000; val_acc: 0.238000\n",
      "(Epoch 4 / 25) train acc: 0.434000; val_acc: 0.249000\n",
      "(Epoch 5 / 25) train acc: 0.486000; val_acc: 0.262000\n",
      "(Epoch 6 / 25) train acc: 0.472000; val_acc: 0.269000\n",
      "(Epoch 7 / 25) train acc: 0.472000; val_acc: 0.277000\n",
      "(Epoch 8 / 25) train acc: 0.486000; val_acc: 0.277000\n",
      "(Epoch 9 / 25) train acc: 0.486000; val_acc: 0.271000\n",
      "(Epoch 10 / 25) train acc: 0.480000; val_acc: 0.270000\n",
      "(Epoch 11 / 25) train acc: 0.476000; val_acc: 0.263000\n",
      "(Epoch 12 / 25) train acc: 0.474000; val_acc: 0.264000\n",
      "(Epoch 13 / 25) train acc: 0.474000; val_acc: 0.263000\n",
      "(Epoch 14 / 25) train acc: 0.474000; val_acc: 0.262000\n",
      "(Epoch 15 / 25) train acc: 0.472000; val_acc: 0.263000\n",
      "(Epoch 16 / 25) train acc: 0.472000; val_acc: 0.262000\n",
      "(Epoch 17 / 25) train acc: 0.474000; val_acc: 0.262000\n",
      "(Epoch 18 / 25) train acc: 0.474000; val_acc: 0.262000\n",
      "(Epoch 19 / 25) train acc: 0.474000; val_acc: 0.262000\n",
      "(Epoch 20 / 25) train acc: 0.474000; val_acc: 0.263000\n",
      "(Iteration 101 / 125) loss: 0.000011\n",
      "(Epoch 21 / 25) train acc: 0.474000; val_acc: 0.263000\n",
      "(Epoch 22 / 25) train acc: 0.474000; val_acc: 0.263000\n",
      "(Epoch 23 / 25) train acc: 0.474000; val_acc: 0.263000\n",
      "(Epoch 24 / 25) train acc: 0.474000; val_acc: 0.263000\n",
      "(Epoch 25 / 25) train acc: 0.474000; val_acc: 0.263000\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Agath\\OneDrive\\Documents\\SemestreHiver2020\\IFT725\\TP2_IFT725\\prog\\ift725\\layers.py:606: RuntimeWarning: divide by zero encountered in log\n",
      "  good_scores = -np.log(good_scores)\n",
      "C:\\Users\\Agath\\OneDrive\\Documents\\SemestreHiver2020\\IFT725\\TP2_IFT725\\prog\\ift725\\layers.py:604: RuntimeWarning: overflow encountered in exp\n",
      "  probs = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
      "C:\\Users\\Agath\\OneDrive\\Documents\\SemestreHiver2020\\IFT725\\TP2_IFT725\\prog\\ift725\\layers.py:604: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probs = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
      "C:\\Users\\Agath\\OneDrive\\Documents\\SemestreHiver2020\\IFT725\\TP2_IFT725\\prog\\ift725\\layers.py:107: RuntimeWarning: invalid value encountered in greater\n",
      "  cache = x > 0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Train two identical nets, one with dropout and one without\n",
    "\n",
    "num_train = 500\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "dropout_choices = [0, 0.25]\n",
    "for dropout in dropout_choices:\n",
    "  model = FullyConnectedNeuralNet([500], dropout=dropout)\n",
    "  print(dropout)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=25, batch_size=100,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 5e-4,\n",
    "                  },\n",
    "                  verbose=True, print_every=100)\n",
    "  solver.train()\n",
    "  solvers[dropout] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot train and validation accuracies of the two models\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for dropout in dropout_choices:\n",
    "  solver = solvers[dropout]\n",
    "  train_accs.append(solver.train_acc_history[-1])\n",
    "  val_accs.append(solver.val_acc_history[-1])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "for dropout in dropout_choices:\n",
    "  plt.plot(solvers[dropout].train_acc_history, '-', label='%.2f dropout' % dropout)\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for dropout in dropout_choices:\n",
    "  plt.plot(solvers[dropout].val_acc_history, '-', label='%.2f dropout' % dropout)\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "Expliquez la conclusion que l'on peut tirer de ces deux courbes.\n",
    "\n",
    "## Votre réponse : ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}